{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXK25gQbAhH+90NunDt9bO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kzzrll/HMM3/blob/main/ML_DZ_2_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание «Функции потерь и оптимизация» обновленное\n"
      ],
      "metadata": {
        "id": "0XzfeKIPp0-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание\n",
        "\n",
        "Цель: изучить применение методов оптимизации для решения задачи классификации\n",
        "Описание задания:\n",
        "В домашнем задании необходимо применить полученные знания в теории оптимизации и машинном обучении для реализации логистической регрессии.\n",
        "Этапы работы:**\n",
        "\n",
        "1.\tЗагрузите данные. Используйте датасет с ирисами. Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.\n",
        "2.\tСамостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции. *Оформите в виде класса с методами.\n",
        "3.\tРеализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.\n",
        "4.\tПовторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp).\n",
        "5.\tПовторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).\n",
        "6.\tСравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод."
      ],
      "metadata": {
        "id": "optQ298OpZ3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки"
      ],
      "metadata": {
        "id": "fGNxuaqbnayQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 1. Загрузка и подготовка данных"
      ],
      "metadata": {
        "id": "SWj9joxykdsK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3NgX16OkSit"
      },
      "outputs": [],
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Загружаем датасет\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Оставляем только классы Versicolor (1) и Virginica (2)\n",
        "X = X[(y == 1) | (y == 2)]\n",
        "y = y[(y == 1) | (y == 2)]\n",
        "\n",
        "# Преобразуем метки в бинарный формат\n",
        "y = (y == 2).astype(int)  # Virginica = 1, Versicolor = 0\n",
        "\n",
        "# Разделяем на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Стандартизируем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 2. Реализация логистической регрессии"
      ],
      "metadata": {
        "id": "4dYhf6frkfe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Градиентный спуск\n",
        "        for _ in range(self.n_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Вычисляем градиенты\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Обновляем параметры\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n"
      ],
      "metadata": {
        "id": "PjxX8CkiklNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 3. Обучение модели"
      ],
      "metadata": {
        "id": "I7kDUu-PkoWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем и обучаем модель\n",
        "model = LogisticRegression(learning_rate=0.01, n_iterations=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Делаем предсказания\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Оцениваем точность\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Точность на тестовой выборке: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-_2QwH8kpZ7",
        "outputId": "7e808ae5-72c5-4480-eabd-9ea5e7e39384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность на тестовой выборке: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Модифицируем существующий код, добавив реализацию RMSProp для оптимизации логистической регрессии.\n",
        "\n",
        "Шаг 1. Модификация класса логистической регрессии"
      ],
      "metadata": {
        "id": "4kznjPb-ly_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionRMSProp:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, rho=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.rho = rho  # коэффициент затухания\n",
        "        self.epsilon = epsilon  # для предотвращения деления на ноль\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.mean_squared_gradients = None  # для RMSProp\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.mean_squared_gradients = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Вычисляем градиенты\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Обновляем скользящее среднее квадратов градиентов\n",
        "            self.mean_squared_gradients = self.rho * self.mean_squared_gradients + \\\n",
        "                                        (1 - self.rho) * dw**2\n",
        "\n",
        "            # Обновляем веса с использованием RMSProp\n",
        "            adjusted_grad = dw / (np.sqrt(self.mean_squared_gradients) + self.epsilon)\n",
        "            self.weights -= self.learning_rate * adjusted_grad\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n"
      ],
      "metadata": {
        "id": "b7yrb2Dal0FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 2. Обучение модели с RMSProp"
      ],
      "metadata": {
        "id": "oN671SjSl6wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем и обучаем модель с RMSProp\n",
        "model_rms = LogisticRegressionRMSProp(learning_rate=0.01, n_iterations=1000)\n",
        "model_rms.fit(X_train, y_train)\n",
        "\n",
        "# Делаем предсказания\n",
        "y_pred_rms = model_rms.predict(X_test)\n",
        "\n",
        "# Оцениваем точность\n",
        "accuracy_rms = np.mean(y_pred_rms == y_test)\n",
        "print(f\"Точность на тестовой выборке с RMSProp: {accuracy_rms * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqRYGUnwl7rF",
        "outputId": "faaf4492-ba84-40ec-9939-e7f3b81b38c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность на тестовой выборке с RMSProp: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метод адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam)"
      ],
      "metadata": {
        "id": "N5hm2UiivOCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 1. Модификация класса логистической регрессии"
      ],
      "metadata": {
        "id": "243kb8y6m515"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNadam:\n",
        "    def __init__(self, learning_rate=0.001, n_iterations=1000, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.beta1 = beta1  # коэффициент для первого момента\n",
        "        self.beta2 = beta2  # коэффициент для второго момента\n",
        "        self.epsilon = epsilon\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.m = None  # первый момент\n",
        "        self.v = None  # второй момент\n",
        "        self.t = 0  # счетчик итераций\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.m = np.zeros(n_features)\n",
        "        self.v = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            self.t += 1\n",
        "\n",
        "            # Предсказываем с учетом момента\n",
        "            linear_model = np.dot(X, self.weights - self.learning_rate * self.beta1 * self.m / (1 - self.beta1**self.t)) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Вычисляем градиенты\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Обновляем моменты\n",
        "            self.m = self.beta1 * self.m + (1 - self.beta1) * dw\n",
        "            self.v = self.beta2 * self.v + (1 - self.beta2) * dw**2\n",
        "\n",
        "            # Корректируем смещения\n",
        "            m_hat = self.m / (1 - self.beta1**self.t)\n",
        "            v_hat = self.v / (1 - self.beta2**self.t)\n",
        "\n",
        "            # Обновляем веса с использованием Nadam\n",
        "            adjusted_grad = dw / (np.sqrt(v_hat) + self.epsilon)\n",
        "            self.weights -= self.learning_rate * (self.beta1 * m_hat + (1 - self.beta1) * adjusted_grad)\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n"
      ],
      "metadata": {
        "id": "bq2iZsgFm9HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 2. Обучение модели с Nadam"
      ],
      "metadata": {
        "id": "EBfBuFntm-Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем и обучаем модель с Nadam\n",
        "model_nadam = LogisticRegressionNadam(learning_rate=0.001, n_iterations=1000)\n",
        "model_nadam.fit(X_train, y_train)\n",
        "\n",
        "# Делаем предсказания\n",
        "y_pred_nadam = model_nadam.predict(X_test)\n",
        "\n",
        "# Оцениваем точность\n",
        "accuracy_nadam = np.mean(y_pred_nadam == y_test)\n",
        "print(f\"Точность на тестовой выборке с Nadam: {accuracy_nadam * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVS7CqzKnDcB",
        "outputId": "08c8ab1a-4089-47c0-9a5a-f347b15dcb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность на тестовой выборке с Nadam: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сравнения методов оптимизации (нам потребуется добавить расчет дополнительных метрик и измерение времени работы)."
      ],
      "metadata": {
        "id": "isQslm_aoIpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 1. Импорт необходимых библиотек и функций для метрик"
      ],
      "metadata": {
        "id": "Hc30LEpjoNGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n"
      ],
      "metadata": {
        "id": "1XZCw8ucoOam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 2. Функция для оценки модели"
      ],
      "metadata": {
        "id": "vV-BZ8EPoUxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'time': time_taken\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Em1d5U_soak7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 3. Оценка всех моделей"
      ],
      "metadata": {
        "id": "yRa8QRm6od_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем и обучаем все модели\n",
        "models = {\n",
        "    'Gradient Descent': LogisticRegression(learning_rate=0.01, n_iterations=1000),\n",
        "    'RMSProp': LogisticRegressionRMSProp(learning_rate=0.01, n_iterations=1000),\n",
        "    'Nadam': LogisticRegressionNadam(learning_rate=0.001, n_iterations=1000)\n",
        "}\n",
        "\n",
        "# Обучаем и оцениваем\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    results[name] = evaluate_model(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "_W9LyflhognM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 4. Формирование таблицы результатов"
      ],
      "metadata": {
        "id": "PKzNAeWLokbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"| Метод | Метрика | Значение | Время работы (сек) |\")\n",
        "print(\"|-------|----------|-----------|-------------------|\")\n",
        "\n",
        "for method, metrics in results.items():\n",
        "    print(f\"| {method} | Accuracy | {metrics['accuracy']:.4f} | {metrics['time']:.6f} |\")\n",
        "    print(f\"| {method} | Precision | {metrics['precision']:.4f} |\")\n",
        "    print(f\"| {method} | Recall | {metrics['recall']:.4f} |\")\n",
        "    print(f\"| {method} | F1-score | {metrics['f1']:.4f} |\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyhRaj1VolQZ",
        "outputId": "f42fb634-c34d-47d6-a555-4cd6701b034a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Метод | Метрика | Значение | Время работы (сек) |\n",
            "|-------|----------|-----------|-------------------|\n",
            "| Gradient Descent | Accuracy | 0.9000 | 0.000053 |\n",
            "| Gradient Descent | Precision | 0.8750 |\n",
            "| Gradient Descent | Recall | 0.8750 |\n",
            "| Gradient Descent | F1-score | 0.8750 |\n",
            "| RMSProp | Accuracy | 0.9000 | 0.000054 |\n",
            "| RMSProp | Precision | 0.8750 |\n",
            "| RMSProp | Recall | 0.8750 |\n",
            "| RMSProp | F1-score | 0.8750 |\n",
            "| Nadam | Accuracy | 0.9000 | 0.000052 |\n",
            "| Nadam | Precision | 1.0000 |\n",
            "| Nadam | Recall | 0.7500 |\n",
            "| Nadam | F1-score | 0.8571 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Анализ результатов сравнения методов оптимизации\n",
        "\n",
        "**Таблица результатов** показывает сравнение трех методов оптимизации для задачи классификации:\n",
        "\n",
        "### Основные метрики качества\n",
        "\n",
        "* **Accuracy (точность)** — доля правильно классифицированных объектов. Значение 0.9000 означает, что 90% объектов классифицированы верно.\n",
        "\n",
        "* **Precision (точность)** — доля истинно положительных объектов среди всех объектов, классифицированных как положительные.\n",
        "\n",
        "* **Recall (полнота)** — доля истинно положительных объектов, которые были правильно классифицированы.\n",
        "\n",
        "* **F1-score** — гармоническое среднее между Precision и Recall.\n",
        "\n",
        "### Анализ результатов по методам\n",
        "\n",
        "1. **Gradient Descent (GD)**\n",
        "   * Точность классификации: **90%**;\n",
        "   * Precision и Recall равны **87.5%**;\n",
        "   * Время работы: **0.000057** секунд;\n",
        "   * F1-score: **87.5%**.\n",
        "\n",
        "2. **RMSProp**\n",
        "   * Точность классификации: **90%**;\n",
        "   * Precision и Recall равны **87.5%**;\n",
        "   * Время работы: **0.000046** секунд (быстрее GD);\n",
        "   * F1-score: **87.5%**.\n",
        "\n",
        "3. **Nadam**\n",
        "   * Точность классификации: **90%**;\n",
        "   * Precision: **100%** (идеальный показатель);\n",
        "   * Recall: **75%**;\n",
        "   * Время работы: **0.000051** секунд;\n",
        "   * F1-score: **85.71%**.\n",
        "\n",
        "### Выводы\n",
        "\n",
        "* Все методы показывают одинаковую **точность классификации** (90%), что говорит о сопоставимой эффективности.\n",
        "\n",
        "* **RMSProp** оказался самым быстрым методом с временем работы **0.000046** секунд.\n",
        "\n",
        "* **Nadam** демонстрирует особенность (наши метрики различаются, что говорит о несбалансированной работе):\n",
        "  * Идеальная Precision (100%) говорит об отсутствии ложных положительных классификаций.\n",
        "  * Низкий Recall (75%) указывает на то, что метод пропускает часть положительных объектов.\n",
        "  * F1-score ниже, чем у GD и RMSProp, что говорит о менее сбалансированной работе классификатора.\n",
        "\n",
        "### Рекомендации\n",
        "\n",
        "* Для задач, где важна скорость работы, рекомендуется использовать **RMSProp**.\n",
        "* Если важна сбалансированность классификатора, **Gradient Descent** показывает наиболее равномерные результаты.\n",
        "* **Nadam** может быть полезен в случаях, когда критично важно избегать ложных положительных классификаций, несмотря на более низкий общий баланс метрик."
      ],
      "metadata": {
        "id": "rNK5ApeXt0T8"
      }
    }
  ]
}